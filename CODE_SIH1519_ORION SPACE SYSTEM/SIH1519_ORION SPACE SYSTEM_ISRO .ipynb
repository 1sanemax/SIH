{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/nacdem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m dem_dir_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/nacdem\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m dem_dir_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/tmcdtm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 39\u001b[0m dem_files_1 \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dem_dir_1, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdem_dir_1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     40\u001b[0m dem_files_2 \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dem_dir_2, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dem_dir_2) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     42\u001b[0m preprocessed_dem1 \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/nacdem'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "# Function to preprocess DEM data\n",
    "def preprocess_dem(dem):\n",
    "    # Normalize the DEM data (values between 0 and 1)\n",
    "    normalized_dem = dem / np.max(dem)\n",
    "    # Scale the DEM data to 0-255 for visualization\n",
    "    scaled_dem = (normalized_dem * 255).astype(np.uint8)\n",
    "    return scaled_dem\n",
    "\n",
    "# Function to identify slopes using edge Canny detection\n",
    "def identify_slopes(dem):\n",
    "    # Perform edge detection using Canny on the preprocessed DEM\n",
    "    edges = cv2.Canny(dem, 50, 150)\n",
    "\n",
    "    # Calculate gradient using Sobel operator\n",
    "    gradient_x = cv2.Sobel(dem, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    gradient_y = cv2.Sobel(dem, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    # Calculate magnitude and angle of the gradient\n",
    "    magnitude = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "    angle = np.arctan2(gradient_y, gradient_x) * (180 / np.pi)\n",
    "\n",
    "    # Set threshold for slopes greater than 10 degrees\n",
    "    slope_threshold = 10\n",
    "    steep_slope_mask = np.zeros_like(dem)\n",
    "    steep_slope_mask[np.abs(angle) > slope_threshold] = 255  # Using grayscale value for visualization\n",
    "\n",
    "    return steep_slope_mask  # Return the mask for slopes greater than 10 degrees\n",
    "\n",
    "# Load DEM datasets using rasterio\n",
    "dem_dir_1 = '/kaggle/input/nacdem'\n",
    "dem_dir_2 = '/kaggle/input/tmcdtm'\n",
    "\n",
    "dem_files_1 = [os.path.join(dem_dir_1, f) for f in os.listdir(dem_dir_1) if f.endswith('.tif')]\n",
    "dem_files_2 = [os.path.join(dem_dir_2, f) for f in os.listdir(dem_dir_2) if f.endswith('.tif')]\n",
    "\n",
    "preprocessed_dem1 = []\n",
    "preprocessed_dem2 = []\n",
    "\n",
    "# Load and preprocess DEM datasets\n",
    "for dem_file in dem_files_1:\n",
    "    with rasterio.open(dem_file) as dem:\n",
    "        dem_array = dem.read(1)\n",
    "        preprocessed_dem1.append(preprocess_dem(dem_array))\n",
    "\n",
    "for dem_file in dem_files_2:\n",
    "    with rasterio.open(dem_file) as dem:\n",
    "        dem_array = dem.read(1)\n",
    "        preprocessed_dem2.append(preprocess_dem(dem_array))\n",
    "\n",
    "# Identify slopes for both DEM datasets\n",
    "slope_masks_1 = [identify_slopes(dem) for dem in preprocessed_dem1]\n",
    "slope_masks_2 = [identify_slopes(dem) for dem in preprocessed_dem2]\n",
    "\n",
    "# Display original images and sample slope images for both datasets with user-defined figure sizes\n",
    "num_samples = 1\n",
    "figsize_1 = (15, 6)  # Change figure size for DEM dataset 1\n",
    "figsize_2 = (40, 20)  # Change figure size for DEM dataset 2\n",
    "\n",
    "plt.figure(figsize=figsize_1)\n",
    "\n",
    "# For DEM dataset 1\n",
    "for i in range(num_samples):\n",
    "    with rasterio.open(dem_files_1[i]) as dem:\n",
    "        image = dem.read(1)\n",
    "        plt.subplot(3, num_samples, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'DEM 1 Original {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, num_samples, num_samples + i + 1)\n",
    "    plt.imshow(slope_masks_1[i], cmap='gray')  # Grayscale colormap for better edge detection\n",
    "    plt.title(f'DEM 1 Sample Slope {i+1}')\n",
    "    plt.axis('off')\n",
    "    # Save slope mask for DEM dataset 1\n",
    "    plt.imsave(f'dem1_slope_{i+1}.png', slope_masks_1[i], cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize_2)\n",
    "\n",
    "# For DEM dataset 2\n",
    "for i in range(num_samples):\n",
    "    with rasterio.open(dem_files_2[i]) as dem:\n",
    "        image = dem.read(1)\n",
    "        plt.subplot(3, num_samples, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'DEM 2 Original {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, num_samples, num_samples + i + 1)\n",
    "    plt.imshow(slope_masks_2[i], cmap='gray')  # Grayscale colormap for better edge detection\n",
    "    plt.title(f'DEM 2 Sample Slope {i+1}')\n",
    "    plt.axis('off')\n",
    "    # Save slope mask for DEM dataset 2\n",
    "    plt.imsave(f'dem2_slope_{i+1}.png', slope_masks_2[i], cmap='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "# Function to detect shadows using intensity comparison\n",
    "def detect_shadows(image):\n",
    "    # Calculate a threshold based on intensity (adjust the value as needed)\n",
    "    threshold_value = 150  # Adjust this threshold value for shadow detection\n",
    "    _, thresh = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "# Load ORTH datasets using rasterio\n",
    "orth_dir_1 = '/kaggle/input/orthonac123/ORTHONAC0.5-1/1'\n",
    "orth_dir_2 = '/kaggle/input/tmcorth/TMCORTHO5-1/1'\n",
    "\n",
    "orth_files_1 = [os.path.join(orth_dir_1, f) for f in os.listdir(orth_dir_1) if f.endswith('.tif')]\n",
    "orth_files_2 = [os.path.join(orth_dir_2, f) for f in os.listdir(orth_dir_2) if f.endswith('.tif')]\n",
    "\n",
    "# Process images and detect shadows\n",
    "shadow_masks_1 = []\n",
    "shadow_masks_2 = []\n",
    "\n",
    "for orth_file in orth_files_1:\n",
    "    with rasterio.open(orth_file) as orth:\n",
    "        image = orth.read(1)  # Read a single-band grayscale image\n",
    "        # Detect shadows in the image\n",
    "        shadow_mask = detect_shadows(image)\n",
    "        shadow_masks_1.append(shadow_mask)\n",
    "\n",
    "for orth_file in orth_files_2:\n",
    "    with rasterio.open(orth_file) as orth:\n",
    "        image = orth.read(1)  # Read a single-band grayscale image\n",
    "        # Detect shadows in the image\n",
    "        shadow_mask = detect_shadows(image)\n",
    "        shadow_masks_2.append(shadow_mask)\n",
    "\n",
    "# Display sample images with shadow masks\n",
    "num_samples = 1\n",
    "figsize_1 = (7, 6)  # Change figure size for ORTH dataset 1\n",
    "figsize_2 = (40, 20)  # Change figure size for ORTH dataset 2\n",
    "\n",
    "plt.figure(figsize=figsize_1)\n",
    "\n",
    "# For ORTH dataset 1\n",
    "for i in range(num_samples):\n",
    "    with rasterio.open(orth_files_1[i]) as orth:\n",
    "        image = orth.read(1)  # Read a single-band grayscale image\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title('Original Image 1')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "        plt.imshow(shadow_masks_1[i], cmap='gray')\n",
    "        plt.title('Shadow Mask 1')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save shadow mask\n",
    "        plt.imsave(f'shadow_mask_1_{i}.png', shadow_masks_1[i], cmap='gray')  # Save the shadow mask\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize_2)\n",
    "\n",
    "# For ORTH dataset 2\n",
    "for i in range(num_samples):\n",
    "    with rasterio.open(orth_files_2[i]) as orth:\n",
    "        image = orth.read(1)  # Read a single-band grayscale image\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title('Original Image 2')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "        plt.imshow(shadow_masks_2[i], cmap='gray')\n",
    "        plt.title('Shadow Mask 2')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save shadow mask\n",
    "        plt.imsave(f'shadow_mask_2_{i}.png', shadow_masks_2[i], cmap='gray')  # Save the shadow mask\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A  # our data augmentation library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove arnings (optional)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset is in cocoformat, we will need pypcoco tools\n",
    "!pip install pycocotools\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will define our transforms\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(600, 600), # our input size can be 600px\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(600, 600), # our input size can be 600px\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AquariumDetection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        # the 3 transform parameters are reuqired for datasets.VisionDataset\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split #train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "    \n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target] # required annotation format for albumentations\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "        \n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "        \n",
    "        new_boxes = [] # convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "        \n",
    "        targ = {} # here is our transformed target\n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ # scale images\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input/yolov7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load classes\n",
    "coco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [i[1]['name'] for i in categories.items()]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_display = 5  # Set the number of samples to display\n",
    "\n",
    "fig, axes = plt.subplots(1, num_samples_to_display, figsize=(20, 10))\n",
    "\n",
    "for i in range(num_samples_to_display):\n",
    "    sample = train_dataset[i]  # Access each sample from the dataset\n",
    "    img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)  # Convert image to tensor\n",
    "\n",
    "    # Plot each image with bounding boxes\n",
    "    axes[i].imshow(draw_bounding_boxes(\n",
    "        img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n",
    "    ).permute(1, 2, 0))\n",
    "    axes[i].axis('off')  # Turn off axis labels\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the faster rcnn model\n",
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,targets = next(iter(train_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets) # just make sure this runs without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, and optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 22], gamma=0.1) # lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "#     lr_scheduler = None\n",
    "#     if epoch == 0:\n",
    "#         warmup_factor = 1.0 / 1000 # do lr warmup\n",
    "#         warmup_iters = min(1000, len(loader) - 1)\n",
    "        \n",
    "#         lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = warmup_factor, total_iters=warmup_iters)\n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets) # the model computes the loss automatically if we pass in targets\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if lr_scheduler is not None:\n",
    "#             lr_scheduler.step() # \n",
    "        \n",
    "    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n",
    "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
    "        all_losses_dict['loss_classifier'].mean(),\n",
    "        all_losses_dict['loss_box_reg'].mean(),\n",
    "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
    "        all_losses_dict['loss_objectness'].mean()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    # Assuming lr_scheduler is active\n",
    "    # lr_scheduler.step() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AquariumDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_images_to_display = 5\n",
    "count_displayed = 0\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    img, _ = test_dataset[i]\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "        pred = prediction[0]\n",
    "\n",
    "    if count_displayed < num_images_to_display and len(pred['labels']) > 0:\n",
    "        boxes_to_plot = pred['boxes'][pred['scores'] > 0.3]\n",
    "        labels_to_plot = [classes[i] for i in pred['labels'][pred['scores'] > 0.2].tolist()]\n",
    "\n",
    "        if len(labels_to_plot) != len(boxes_to_plot):\n",
    "            print(f\"Number of boxes and labels mismatch for image {i}\")\n",
    "            labels_to_plot = labels_to_plot[:len(boxes_to_plot)]\n",
    "\n",
    "        img_int = torch.tensor(img * 255, dtype=torch.uint8)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        ax.imshow(draw_bounding_boxes(img_int, boxes_to_plot, labels_to_plot, width=4).permute(1, 2, 0))\n",
    "        ax.set_title('Image with Bounding Boxes')\n",
    "        ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        crater_masks = []\n",
    "        boulder_masks = []\n",
    "        for bbox, label in zip(boxes_to_plot, labels_to_plot):\n",
    "            xmin, ymin, xmax, ymax = map(int, bbox.tolist())\n",
    "            mask = np.zeros((img_int.shape[1], img_int.shape[2]), dtype=np.uint8)\n",
    "            if label == 'crater':  # Detect and create a mask for craters\n",
    "                mask[ymin:ymax, xmin:xmax] = 1\n",
    "                crater_masks.append(mask)\n",
    "            elif label == 'boulder':  # Detect and create a mask for boulders\n",
    "                mask[ymin:ymax, xmin:xmax] = 1\n",
    "                boulder_masks.append(mask)\n",
    "\n",
    "        num_masks_to_display = len(crater_masks) + len(boulder_masks)\n",
    "        fig, axes = plt.subplots(1, num_masks_to_display + 1, figsize=(20, 8))\n",
    "        axes[0].imshow(img_int.permute(1, 2, 0))\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        for j, mask in enumerate(crater_masks + boulder_masks):\n",
    "            axes[j + 1].imshow(mask, cmap='gray')\n",
    "            axes[j + 1].set_title(f'Mask {j+1}')\n",
    "            axes[j + 1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        count_displayed += 1\n",
    "\n",
    "    if count_displayed >= num_images_to_display:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2  # Number of samples to consider\n",
    "\n",
    "# Assuming masks are available\n",
    "if slope_masks_1 and shadow_masks_1:\n",
    "    resized_slope_masks = [cv2.resize(mask, (img_int.shape[2], img_int.shape[1])) for mask in slope_masks_1]\n",
    "    resized_shadow_masks = [cv2.resize(mask, (img_int.shape[2], img_int.shape[1])) for mask in shadow_masks_1]\n",
    "\n",
    "if slope_masks_2 and shadow_masks_2:\n",
    "    resized_slope_masks_2 = [cv2.resize(mask, (img_int.shape[2], img_int.shape[1])) for mask in slope_masks_2]\n",
    "    resized_shadow_masks_2 = [cv2.resize(mask, (img_int.shape[2], img_int.shape[1])) for mask in shadow_masks_2]\n",
    "    \n",
    "    # Combine slope and shadow masks for Dataset 2\n",
    "    combined_masks_2 = []\n",
    "    for slope_mask, shadow_mask in zip(resized_slope_masks_2, resized_shadow_masks_2):\n",
    "        # Combine masks by logical OR operation\n",
    "        combined_mask = np.maximum(slope_mask, shadow_mask)\n",
    "        combined_masks_2.append(combined_mask)\n",
    "\n",
    "    # Display the combined hazard map for Dataset 2\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(combined_masks_2[i], cmap='RdYlGn')  \n",
    "        plt.title('Combined Hazard Map (Dataset 2)')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Slope or Shadow masks for Dataset 2 are not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "def quit_program():\n",
    "    root.destroy()\n",
    "\n",
    "def open_image():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.png;*.jpg;*.jpeg;*.gif\")])\n",
    "    if file_path:\n",
    "        print(f\"Selected Image: {file_path}\")\n",
    "        display_image(file_path)\n",
    "\n",
    "def display_image(file_path):\n",
    "    # Create a new frame\n",
    "    new_frame = tk.Toplevel(root)\n",
    "    new_frame.title(\"Selected Image\")\n",
    "\n",
    "    # Display the image in the new frame\n",
    "    image = Image.open(file_path)\n",
    "    image.thumbnail((300, 300))  # Resize the image for display\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "    image_label = tk.Label(new_frame, image=photo)\n",
    "    txt_=tk.Label(new_frame,text=file_path)\n",
    "    txt_.pack()\n",
    "    image_label.image = photo\n",
    "    image_label.pack()\n",
    "\n",
    "def create_new_frame():\n",
    "    new_frame = tk.Toplevel(root)\n",
    "    new_frame.title(\"Hazard Map Generation Precision Lunar Lander\")\n",
    "    def quit_page():\n",
    "        new_frame.destroy()\n",
    "    image = Image.open(\"D:\\SIH\\dem1.jpg\")  \n",
    "    image.thumbnail((400, 400))\n",
    "    photo = ImageTk.PhotoImage(image)\n",
    "    image_label = Label(new_frame, image=photo)\n",
    "    image_label.photo = photo\n",
    "    image_label.grid(row=0, column=0, pady=(10, 20), padx=20)\n",
    "    text_label = tk.Label(new_frame, text=\"D:\\SIH\\dem1.jpg\")   \n",
    "    text_label.grid(row=1, column=0, pady=(0, 10), padx=20)\n",
    "\n",
    "    image1 = Image.open(\"D:\\SIH\\dem2.jpg\")   \n",
    "    image1.thumbnail((400, 400))\n",
    "    photo1 = ImageTk.PhotoImage(image1)\n",
    "    image_label1 = Label(new_frame, image=photo1)\n",
    "    image_label1.photo1 = photo1\n",
    "    image_label1.grid(row=0, column=1, pady=(10, 20), padx=20)\n",
    "    text_label1 = tk.Label(new_frame, text=\"D:\\SIH\\dem2.jpg\")   \n",
    "    text_label1.grid(row=1, column=1, pady=(0, 10), padx=20)\n",
    "\n",
    "    image2 = Image.open(\"D:\\SIH\\Shadow1.jpg\")   \n",
    "    image2.thumbnail((400, 400))\n",
    "    photo2 = ImageTk.PhotoImage(image2)\n",
    "    image_label2 = Label(new_frame, image=photo2)\n",
    "    image_label2.photo2 = photo2\n",
    "    image_label2.grid(row=0, column=2, pady=(10, 20), padx=20)\n",
    "    text_label2 = tk.Label(new_frame, text=\"D:\\SIH\\Shadow1.jpg\")   \n",
    "    text_label2.grid(row=1, column=2, pady=(0, 10), padx=20)\n",
    "\n",
    "    image3 = Image.open(\"D:\\SIH\\shadow2.jpg\")   \n",
    "    image3.thumbnail((400, 400))\n",
    "    photo3 = ImageTk.PhotoImage(image3)\n",
    "    image_label3 = Label(new_frame, image=photo3)\n",
    "    image_label3.photo3 = photo3\n",
    "    image_label3.grid(row=2, column=1, pady=(10, 20), padx=20)\n",
    "    text_label3 = tk.Label(new_frame, text=\"D:\\SIH\\shadow2.jpg\")   \n",
    "    text_label3.grid(row=3, column=1, pady=(0, 10), padx=20)\n",
    "\n",
    "    quit_button = Button(new_frame, text=\"EXIT\",padx=50,pady=10, command=quit_program)\n",
    "    quit_button.grid(row=2, column=2, pady=(10, 10), padx=20)\n",
    "\n",
    "    back_button = Button(new_frame, text=\"BACK\",padx=50,pady=10, command=quit_page)\n",
    "    back_button.grid(row=2, column=0, pady=(10, 10), padx=20)\n",
    "\n",
    "def move_gif(frame_num=0):\n",
    "    try:\n",
    "        # Open the GIF and get the next frame\n",
    "        gif_img.seek(frame_num)\n",
    "        gif_frame = ImageTk.PhotoImage(gif_img)\n",
    "\n",
    "        # Update the background image\n",
    "        background_label.config(image=gif_frame)\n",
    "        background_label.image = gif_frame\n",
    "\n",
    "        # Schedule the next frame update\n",
    "        root.after(50, move_gif, frame_num + 1)\n",
    "    except EOFError:\n",
    "        # Restart the GIF loop when it reaches the end\n",
    "        move_gif()\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title('Orion Space Systems')\n",
    "\n",
    "gif_img = Image.open(\"D:\\SIH\\luna1.gif\")  # Replace with your actual GIF file path\n",
    "background_label = tk.Label(root)\n",
    "background_label.place(relwidth=1, relheight=1)\n",
    "move_gif()\n",
    "\n",
    "# Create buttons\n",
    "image_button = tk.Button(root, text=\"UPLOAD IMAGE\", padx=100, pady=40,bg=\"#565650\",fg=\"white\", command=open_image)\n",
    "image_button.grid(row=1, column=1, pady=45, padx=40)\n",
    "\n",
    "tot=tk.Label(root,text=\"ORION SPACE SYSTEMS\\n\\nGENERATION OF HAZARD MAP FOR LANDER\",font=(\"Gotham\",11),bg=\"black\",fg=\"white\",padx=50,pady=30)\n",
    "tot.grid(row=0, column=10, columnspan=7, pady=40,padx=700)\n",
    "\n",
    "toti=tk.Label(root,text=\"UPLOAD AN IMAGE\\n USING THE BUTTON BELOW\\n TO CONDUCT A COMPREHENSIVE ANALYSIS\\n OF THE LUNAR TMC IMAGE,\\n TO IDENTIFY POTENTIAL THREATS AND CREATING\\n AN HAZARD MAP TO STRENGTHEN\\n SAFETY PROTOCOLS FOR THE LANDER.\",font=(\"din condensed\",9),bg=\"#565650\",fg=\"white\",padx=10,pady=30)\n",
    "toti.grid(row=0, column=1, columnspan=7, pady=90,padx=40)\n",
    "\n",
    "button = tk.Button(root, text=\"GENERATE\", padx=110, pady=40,bg=\"#565650\",fg=\"white\", command=create_new_frame)\n",
    "button.grid(row=1, column=10, columnspan=7, pady=12,padx=700)  # Use grid method\n",
    "\n",
    "totn=tk.Label(root,text=\"INITIATE THE DATA PROCESSING BY\\n CLICKING THE 'GENERATE' BUTTON\\n ABOVE TO ANALYZE THE IMPORTED TMC DATA.\\n THIS CRITICAL STEP WILL ENABLE\\n THE CREATION OF A VITAL HAZARD MAP ESSENTIAL\\n FOR THE SAFETY PROTOCOL OF THE LANDER.\",font=(\"din condensed\",9),bg=\"#565650\",fg=\"white\",padx=10,pady=30)\n",
    "totn.grid(row=2, column=10, columnspan=7, pady=1,padx=700)\n",
    "\n",
    "# Run the main loop\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4196245,
     "sourceId": 7244284,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4196436,
     "sourceId": 7244551,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4196481,
     "sourceId": 7244608,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4196487,
     "sourceId": 7244615,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4196615,
     "sourceId": 7244772,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
